---
title: 'P value, Significance, and T-test'
output:
  html_document:
    keep_md: yes
---


# P values, Significance, and T-tests

## What is a P value?

A p value is by definition *the probability of observing at least as extreme as the observed result given that the null hypothesis is true*. 

## Misconceptions about p values

### Misconception 1: The p-value indicates the probability of the null hypothesis being true.
**Reality:** The p-value actually indicates the probability of observing data as extreme as, or more extreme than, what was observed, assuming the null hypothesis is true. It is not a direct probability of the null hypothesis itself.

### Misconception 2: A smaller p-value means a more important or impactful result.
**Reality:** The p-value does not measure the size of an effect or the importance of a result. It only measures the strength of evidence against the null hypothesis. A small p-value indicates that such a result is unlikely to occur if the null hypothesis were true, but says nothing about the practical significance of the finding.

### Misconception 3: The p-value can tell you the chance of a Type I error (false positive).
**Reality:** The p-value itself is not the probability of a false positive; it is conditioned on the null hypothesis being true. The predefined alpha level (like 0.05) is what sets the Type I error rate, not the p-value.

### Misconception 4: If a test statistic falls within the critical region, the null hypothesis can be rejected with 100% certainty.
**Reality:** Even if the test statistic falls in the critical region and we reject the null hypothesis, there is still a chance of a Type I error. We never prove or disprove hypotheses with absolute certainty in statistics; we only assess evidence against a null hypothesis. Again, the *chance* of Type I error occurring is predefined by the alpha value which is assumed by the analyst or statistician. 

### Misconception 5: A non-significant result (p-value above the threshold) means there is no effect or the study was wrong.
**Reality:** A non-significant result simply means there was not enough evidence to reject the null hypothesis given the data. It could be due to a small effect size, insufficient power, or just random chance.

### Misconception 6: The p-value tells you the magnitude of the effect or its clinical importance.
**Reality:** The p-value does not give any information about how large or important an effect is. Other statistics, like the effect size and confidence intervals, are needed to understand the magnitude of an effect.

### Misconception 7: P-values are not affected by sample size.
**Reality:** P-values are highly sensitive to sample size precisely because *standard error (by definition) decreases with sample size*. With very large samples, even tiny, trivial effects can produce very small p-values, while in small samples, even large effects may not be statistically significant (reflected in the code that plots p-values with increasing sample sizes).

### Misconception 8: P-values provide a measure of the probability that the observed data was produced by chance alone.
**Reality:** The p-value does not measure the probability that the observed data was produced by chance alone. It measures how compatible the data are with the null hypothesis.


```{r, load libraries}
# suppressing warnings so they don't knit
suppressWarnings(suppressMessages(
  library(tidyverse),
  library(ggplot2)
))
set.seed(123) # for reproducibility
```
## Code Chunk 1: Showing Critical Regions
In this code chunk we will graphically show how critical regions relate to critical values.
```{r, show critical regions}
# Specify some parameters for our normal distribution
mean <- 0
std_dev <- 1

# normal curve
x <- seq(-4, 4, length.out = 2000)
y <- dnorm(x, mean, std_dev)
data <- data.frame(x = x, y = y)

# Specify a few critical values for one-sided test
critical_value_95 <- 1.96 # α = 0.05
critical_value_90 <- 1.645 # α = 0.1
critical_value_80 <- 1.282 # α = 0.2

# build overlapping (nested) right-tail regions
regions <- bind_rows(
  data %>% mutate(threshold = "80%",  crit = critical_value_80),
  data %>% mutate(threshold = "90%",  crit = critical_value_90),
  data %>% mutate(threshold = "95%",  crit = critical_value_95)
) %>%
  mutate(y_fill = ifelse(x >= crit, y, NA_real_))

ggplot(data, aes(x, y)) +
  geom_line(linewidth = 1) +

  # overlapping shaded regions (draw widest first so narrower ones sit on top)
  geom_area(
    data = regions %>% filter(threshold == "80%"),
    aes(y = y_fill, fill = threshold),
    alpha = 0.25,
    na.rm = TRUE
  ) +
  geom_area(
    data = regions %>% filter(threshold == "90%"),
    aes(y = y_fill, fill = threshold),
    alpha = 0.35,
    na.rm = TRUE
  ) +
  geom_area(
    data = regions %>% filter(threshold == "95%"),
    aes(y = y_fill, fill = threshold),
    alpha = 0.45,
    na.rm = TRUE
  ) +

  # critical lines
  geom_vline(xintercept = critical_value_80, linetype = "dashed") +
  geom_vline(xintercept = critical_value_90, linetype = "dashed") +
  geom_vline(xintercept = critical_value_95, linetype = "dashed") +

  # labels near the lines
  annotate("text", x = critical_value_80 + 0.05, y = 0.12, label = "80%", hjust = 0) +
  annotate("text", x = critical_value_90 + 0.05, y = 0.10, label = "90%", hjust = 0) +
  annotate("text", x = critical_value_95 + 0.05, y = 0.08, label = "95%", hjust = 0) +

  scale_fill_manual(
    name = "Critical region (right tail)",
    values = c("80%" = "gold", "90%" = "orange", "95%" = "red")
  ) +
  ggtitle("Overlapping critical regions (one-sided, right tail)") +
  xlab("Test statistic (Z)") +
  ylab("Density") +
  theme_minimal()
```
- **Visualize Normal Distribution**: Plots a curve to show data distribution (mean=0, std dev=1).
- **Critical Region**: Shades area past critical value  to denote the critical region at a 95%, 90%, and 80% confidence level.
- **P-Value**: Highlights the p-value area, illustrating the probability of results as extreme as observed under the null hypothesis.
```{r}
 # Simulate 10000 p-values under the null hypothesis
p_values <- replicate(10000, {
  # 30 samples from a standard normal distribution (mean=0, sd=1)
  sample_data <- rnorm(30, mean, std_dev)
  test_result <- t.test(sample_data)
  test_result$p.value
})

# Plot the distribution of p-values
hist(p_values, breaks=40, main="Distribution of p-values under the Null Hypothesis", xlab="p-value")
abline(v=0.05, col="red", lwd=2, lty=2)

```
- **Simulate P-Values**: Generates 10,000 p-values from simulated experiments under the null hypothesis using a standard normal distribution (mean=0, sd=1) for 30 samples each.

- **T-Test**: Applies a t-test to each set of 30 samples to calculate p-values.

- **Plot Distribution**: Creates a histogram to visualize the distribution of the 10,000 p-values.

- **Highlight Significance Threshold**: Marks the 0.05 significance level with a red dashed line, indicating the conventional cutoff for statistical significance.

- This  emphasizes that under the null hypothesis, p-values are uniformly distributed. This uniform distribution is critical for understanding why a p-value of 0.05 is used as a threshold for significance, indicating that if the null hypothesis is true, 5% of studies will erroneously find significant results purely by chance.

- This also highlights the importance of understanding that a significant p-value does not confirm the alternative hypothesis but rather suggests that the observed data is unlikely under the assumption of the null hypothesis.


## What this code chunk does

- **Generate Distributions**: Creates normal distributions for null (mean=-2, sd=1) and alternative (mean=1, sd=1) hypotheses.

- **Critical Value**: Calculates critical value for a right-tailed test at the 95th percentile of the null distribution.

- **P-value Area**: Highlights the area under the null distribution beyond the critical value, representing the p-value.

- **Visualization**: Uses ggplot for plotting, differentiating distributions with colors and marking critical value and p-value area.


```{r}
# Generating data for null and alternative hypothesis distributions
x_values <- seq(-4, 4, length.out = 1000)
null_distribution <- dnorm(x_values, mean = 0, sd = 1)
alt_distribution <- dnorm(x_values, mean = 1, sd = 1)
critical_value <- qnorm(0.95, mean = 0, sd = 1)  # For a right-tailed test
p_value_area_x <- seq(critical_value, 4, length.out = 1000)
p_value_area_y <- dnorm(p_value_area_x, mean = 0, sd = 1)

# Creating a data frame for ggplot
data <- data.frame(x_values, null_distribution, alt_distribution)

# Plotting
ggplot() +
  geom_line(data = data, aes(x = x_values, y = null_distribution, color = "Null Hypothesis Distribution"), size = 1) +
  geom_line(data = data, aes(x = x_values, y = alt_distribution, color = "Alternative Hypothesis Distribution"), size = 1) +
  geom_area(data = data.frame(p_value_area_x, p_value_area_y), aes(x = p_value_area_x, y = p_value_area_y), fill = "skyblue", alpha = 0.5) +
  geom_vline(xintercept = critical_value, linetype = "dashed", color = "grey") +
  annotate("text", x = critical_value, y = 0.02, label = "Critical Value", hjust = 1) +
  annotate("text", x = critical_value + 2, y = 0.02, label = "P-value Area", color = "blue") +
  scale_color_manual(values = c("Null Hypothesis Distribution" = "skyblue", "Alternative Hypothesis Distribution" = "salmon")) +
  labs(title = "Corrected Visualization of P-value",
       x = "Value",
       y = "Probability Density") +
  theme_minimal() +
  theme(legend.title = element_blank())
```

- **Purpose**: Demonstrates differences between hypotheses, critical value determination, and p-value concept visually.

## What this code chunk does
- **Simulate P-Values for Increasing Sample Sizes**: Generates p-values from t-tests on data with a slight effect (mean=0.5), for sample sizes ranging from 10 to 1000, increasing by 50.

- **Data Preparation**: Creates a data frame mapping each sample size to its corresponding p-value.

- **Plotting**: Utilizes ggplot to plot sample sizes against p-values, incorporating both line and point markers for visualization.

- **Log Scale for P-Values**: Applies a logarithmic scale to the y-axis to more effectively display the range of p-values.


```{r}
# Simulate p-values with increasing sample sizes
sample_sizes <- seq(10, 1000, by = 50)
n_rep <- 500
true_effect <- 0.1

sim_results <- data.frame(
  sample_size = rep(sample_sizes, each = n_rep),
  p_value = unlist(
    lapply(sample_sizes, function(n) {
      replicate(n_rep, {
        x <- rnorm(n, mean = true_effect)
         # note that this t-test is comparing to a normal distribution
         # with mean = 0; sd = 1
        t.test(x)$p.value
      })
    })
  )
)

summary_df <- sim_results |>
  dplyr::group_by(sample_size) |>
  dplyr::summarise(
    p_median = median(p_value),
    p_q25 = quantile(p_value, 0.25),
    p_q75 = quantile(p_value, 0.75),
    .groups = "drop"
  )

# Plot
ggplot(summary_df, aes(x = sample_size, y = p_median)) +
  geom_ribbon(aes(ymin = p_q25, ymax = p_q75), alpha = 0.25) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed") +
  labs(
    title = "P-values vs sample size (replicated experiments)",
    subtitle = "Line = median p-value; band = middle 50% of outcomes",
    x = "Sample size",
    y = "p-value"
  ) +
  theme_minimal()
```
*What did we learn* P values decrease with sample size. *Why?*

Let's assume our data are normally distributed

$$X_1, X_2, ..., X_n \sim N(\mu, \sigma^2) \ (1)$$
When we run a t test we want to know if either the null hypothesis

$$ H_0: \mu = \mu_0 \ (2)$$
or the alternative hypothesis

$$ H_A: \mu \neq \mu_0 \ (3)$$
are true. Right?

To do this we measure the the sample mean:

$$ \bar{X} = \frac{1}{n} \sum_{i=1}^{n}X_i \ (4)$$
and the sample SD:

$$S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2 \ (5)$$
and then we can measure the standard error of the mean (SEM--remember from last lecture?):

$$SE(\bar{X}) = \frac{S}{\sqrt{n}} \ (6)$$
Then we define the *key object* the t statistic:

$$T = \frac{\bar{X} - \mu_0}{\frac{S}{\sqrt{n}}} \ (7)$$
**In words** T is the number of estimated standard errors that the sample mean ($\bar{X}$) is away from the null mean ($\mu_0$). So if $H_0$ is true then the following is true:

$T \sim t_{n-1} (8).$

Again *in words* if $H_0$ is true then the random variable $T$ is a given by a $t$-distribution with $n-1$ degrees of freedom. So under the hood when you run `t.test()` in R, it is simply computing a $t$ statistic:

$$t = \frac{\bar{x}-\mu_0}{\frac{s}{\sqrt{n}}} (9).$$
where the actual sample mean is $\bar{x}$, the sample sd is $s$ and the sample size is $n$. R then asks the question:

$$p = P(|T| \geq |t| \ | \ H_0) \ (10)$$
Or as we said before *how likely is it to see a t statistic (again the number of standard deviations away our observed mean is from the null mean) AT LEAST this extreme*. We can also write this in `R` code.

`p_value <- 2*pt(abs(t). df = n-1, lower.tail = FALSE)`

This says that `R` computes:

- `2` says we want two tails (boths sides of the )
- `pt(x, df)` = a cumulative probability up to `x`.
- `abs(t)` both sides
- `lower.tail = FALSE` tells `R` we want the area to the right of `x`. 

**Take home** Because of equation 10, as $n$ (our sample size) grows our $t$ statistic grows with it. A larger $|t|$ means a smaller tail area. 

::: {style="padding: 15px; margin-bottom: 20px; border: 2px solid #3498db; border-radius: 5px; background-color: #ecf0f1;"}
**Side note**
The code chunk above is actually what's called a *power analysis*. This type of analysis is typically done to determine the minimal sample size to properly measure a particular effect size. 
:::

## What This Code Chunk Does

- **Define Effect Sizes and Sample Size**: Sets up effect sizes ranging from -1.1 to 1.0 (in 0.1 increments) and a constant sample size of 100 for each simulation.

- **Simulate Data and Calculate P-Values**: For each effect size, simulates a control group (mean=0) and a treatment group (mean=d, where d is the effect size), then performs a t-test to compare the two and calculates the p-value.

- **Data Frame Creation**: Organizes effect sizes and their corresponding p-values into a data frame for plotting.

- **Plot Relationship**: Uses ggplot to display the relationship between effect size and p-value.

```{r}
effect_sizes <- seq(0.1, 0.5, by = 0.05)
sample_size <- 100
n_rep <- 500  # number of repeated experiments per effect size

sim_results <- data.frame(
  effect_size = rep(effect_sizes, each = n_rep),
  p_value = unlist(
    lapply(effect_sizes, function(d) {
      replicate(n_rep, {
        control <- rnorm(sample_size, mean = 0)
        treatment <- rnorm(sample_size, mean = d)
        t.test(control, treatment)$p.value
      })
    })
  )
)

summary_df <- sim_results |>
  dplyr::group_by(effect_size) |>
  dplyr::summarise(
    p_median = median(p_value),
    p_q25 = quantile(p_value, 0.25),
    p_q75 = quantile(p_value, 0.75),
    .groups = "drop"
  )

ggplot(summary_df, aes(x = effect_size, y = p_median)) +
  geom_ribbon(aes(ymin = p_q25, ymax = p_q75), alpha = 0.25) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed") +
  labs(
    title = "P-values vs effect size (replicated experiments)",
    subtitle = "Line = median p-value; band = middle 50% of outcomes",
    x = "Effect size",
    y = "p-value"
  ) +
  theme_minimal()

```

*Take home* Our p values decrease *monotonically* with effect size. *Why?* Remember our definition of a $t$ statistic from the prior chunk?

$$t = \frac{\bar{x}-\mu_0}{\frac{s}{\sqrt{n}}} (9).$$
Intuitively Equation 9 tells us that as our sample mean becomes more extreme (farther from the null; which it should if we change the effect size) then our t-statistic (a measure of how far away--how many SDs--our measurement is from the null) shoudl follow suit.

## What the next code chunk does

- **Define T-Values and Sample Sizes**: Specifies t-statistic values from 0 to 6 and a set of distinct sample sizes (3, 4, 5, 10, 20) for analysis.

- **Calculate P-Values**: For each t-value and sample size combination, computes p-values using the t-distribution, doubling the result to account for a two-tailed test.

- **Organize Data**: Arranges t-values, corresponding p-values, and sample sizes into a data frame, preparing it for plotting. Each sample size is treated as a factor to distinguish it in the plot.

- **Plot Relationship**: Utilizes ggplot2 to graph the relationship between t-statistics and p-values.
```{r}
# Define a range of t-values for 0 to 6 and specific sample sizes
sample_sizes <- c(3, 4, 5, 10, 20)
t_values <- seq(0, 6, length.out = 300)

# Calculate p-values for each combination of t-value and sample size
plot_data <- data.frame(
  t_value = rep(t_values, times = length(sample_sizes)),
  p_value = unlist(lapply(sample_sizes, function(n) {
    2 * pt(t_values, df = n - 1, lower.tail = FALSE)
  })),
  sample_size = factor(rep(sample_sizes, each = length(t_values)))
)

# Plot the relationship between p-values and t-statistics
ggplot(plot_data, aes(x = t_value, y = p_value, color = sample_size)) +
  geom_smooth() +
  labs(title = "p-values of t-statistics for Different Sample Sizes",
       x = "t-statistic",
       y = "p-value",
       color = "Sample Size") +
    geom_hline(yintercept = 0.05, linetype = "dashed") +
  theme_minimal()
```
*Take home* This plot shows how the same t-statistic corresponds to different p-values depending on sample size, because the t distribution becomes narrower as degrees of freedom increase. A higher t-statistic can result from a larger effect size, a larger sample size, or smaller sample variance.
```{r}
# Simulate data with a known small effect size under the alternative hypothesis
effect_size <- 0.3
sample_data_null <- rnorm(100) # 100 samples from a standard normal distribution
sample_data_alt <- rnorm(100, mean=effect_size) # 100 samples from a normal distribution with a small effect size

# Conduct a t-test to compare the two groups
test_result <- t.test(sample_data_null, sample_data_alt)
test_result$p.value

# Output the p-value and effect size
cat("p-value:", test_result$p.value, "\nEffect size:", effect_size)

```
```{r}
n_rep <- 1000
true_effect <- 0.5

decisions <- replicate(n_rep, {
  control <- rnorm(100)
  treatment <- rnorm(100, mean = true_effect)
  t.test(control, treatment)$p.value < 0.05
})

mean(decisions)
```
**Take home** When the null is false with effect size 0.5, we reject it about X% of the time. This is the core of *statistical power* or the probability that a statistical hypothesis test rejects the null hypothesis ($H_0$) when the alternative ($H_A$) is true (Dorey 2011). 

## Simple Guides

- **Clarify P-Value Meaning**: Stress that p-values represent the probability of observing the obtained data, or more extreme, under the assumption of the null hypothesis being true. They do not indicate the probability that the null hypothesis is correct or the likelihood of the alternative hypothesis.

- **Understand Limitations of P-Values**: Emphasize that p-values alone cannot measure the magnitude or importance of an effect. They should not be used in isolation to make conclusions about the practical significance of findings. Emphasize the need to consider effect sizes and confidence intervals alongside p-values.

- **Avoid Binary Interpretations**: Discourage the simplistic dichotomy of "significant" vs. "not significant" based on arbitrary p-value thresholds like 0.05. Promote a nuanced interpretation of p-values, recognizing that statistical significance does not equate to scientific or practical importance, and that a higher p-value does not necessarily invalidate the research hypothesis.


## References and Sources
[https://www.youtube.com/watch?v=BZ-Go7VTquk](https://www.youtube.com/watch?v=BZ-Go7VTquk)
Dorey, F. J. 2011. In Brief: Statistics in Brief: Statistical Power: What Is It and When Should It Be Used? Clinical Orthopaedics and Related Research 469:619–620.
